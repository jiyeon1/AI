{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "spektral_gcn.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "0c98jwwdhu_f"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0c98jwwdhu_f"
      },
      "source": [
        "## GCN-Pytorch"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NnqLswXeJt0",
        "outputId": "83db30bf-b7ca-42da-ab91-42e4eab9c48f"
      },
      "source": [
        "!git clone https://github.com/dragen1860/GCN-PyTorch.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'GCN-PyTorch'...\n",
            "remote: Enumerating objects: 72, done.\u001b[K\n",
            "remote: Total 72 (delta 0), reused 0 (delta 0), pack-reused 72\u001b[K\n",
            "Unpacking objects: 100% (72/72), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DXBgihvjeLrq",
        "outputId": "bd8def81-0809-4991-91e7-a190b2f75856"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TldFNTemeNJw",
        "outputId": "753e6585-a159-4c13-ccd9-fa71a13bf453"
      },
      "source": [
        "%cd GCN-PyTorch/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/GCN-PyTorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JnSz7Nn2eQ4y",
        "outputId": "b0033a4f-d159-45b6-eac8-e2ff85c2b077"
      },
      "source": [
        "!python train.py --epochs 400 --dataset pubmed"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(dataset='pubmed', dropout=0.5, early_stopping=10, epochs=400, hidden=16, learning_rate=0.01, max_degree=3, model='gcn', weight_decay=0.0005)\n",
            "adj: (19717, 19717)\n",
            "features: (19717, 500)\n",
            "y: (19717, 3) (19717, 3) (19717, 3)\n",
            "mask: (19717,) (19717,) (19717,)\n",
            "x : tensor(indices=tensor([[    0,     0,     0,  ..., 19716, 19716, 19716],\n",
            "                       [  394,   384,   382,  ...,    16,     7,     1]]),\n",
            "       values=tensor([0.0283, 0.0162, 0.0125,  ..., 0.0104, 0.0030, 0.0145]),\n",
            "       device='cuda:0', size=(19717, 500), nnz=988031, layout=torch.sparse_coo)\n",
            "sp: tensor(indices=tensor([[    0,  1378,  1544,  ..., 19715, 16030, 19716],\n",
            "                       [    0,     0,     0,  ..., 19715, 19716, 19716]]),\n",
            "       values=tensor([0.1667, 0.0671, 0.0680,  ..., 0.5000, 0.2500, 0.5000]),\n",
            "       device='cuda:0', size=(19717, 19717), nnz=108365, layout=torch.sparse_coo)\n",
            "input dim: 500\n",
            "output dim: 3\n",
            "num_features_nonzero: 988031\n",
            "/content/GCN-PyTorch/utils.py:45: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n",
            "  i = i[:, dropout_mask]\n",
            "/content/GCN-PyTorch/utils.py:46: UserWarning: indexing with dtype torch.uint8 is now deprecated, please use a dtype torch.bool instead. (Triggered internally at  /pytorch/aten/src/ATen/native/IndexingUtils.h:30.)\n",
            "  v = v[dropout_mask]\n",
            "0 5.121628761291504 0.4166666865348816\n",
            "10 4.522834777832031 0.4333333373069763\n",
            "20 3.969728946685791 0.44999998807907104\n",
            "30 3.5072829723358154 0.4666666090488434\n",
            "40 3.096879243850708 0.5666666626930237\n",
            "50 2.7279062271118164 0.7666666507720947\n",
            "60 2.4075279235839844 0.8166666626930237\n",
            "70 2.1010541915893555 0.8666666746139526\n",
            "80 1.886613130569458 0.8333333730697632\n",
            "90 1.644883632659912 0.9166666269302368\n",
            "100 1.5148122310638428 0.8666666746139526\n",
            "110 1.349210500717163 0.9333333373069763\n",
            "120 1.1554394960403442 0.9833333492279053\n",
            "130 1.1094331741333008 0.9500000476837158\n",
            "140 0.9638514518737793 0.9833333492279053\n",
            "150 0.9199109077453613 0.9333333373069763\n",
            "160 0.8214528560638428 0.9833333492279053\n",
            "170 0.7129577398300171 0.9666666984558105\n",
            "180 0.7147955298423767 0.9833333492279053\n",
            "190 0.6244296431541443 0.9666666388511658\n",
            "200 0.6221071481704712 0.9833333492279053\n",
            "210 0.5743087530136108 0.9500000476837158\n",
            "220 0.49760901927948 1.0\n",
            "230 0.4917468726634979 0.9833333492279053\n",
            "240 0.4967595338821411 0.9833333492279053\n",
            "250 0.4215891361236572 1.0\n",
            "260 0.42915093898773193 1.0\n",
            "270 0.4103677272796631 1.0\n",
            "280 0.4059215486049652 0.9833333492279053\n",
            "290 0.37670743465423584 1.0\n",
            "300 0.41065409779548645 0.9500000476837158\n",
            "310 0.34967175126075745 1.0\n",
            "320 0.360745906829834 0.9833333492279053\n",
            "330 0.3207513689994812 1.0\n",
            "340 0.3148907423019409 1.0\n",
            "350 0.3262688219547272 1.0\n",
            "360 0.286687970161438 1.0\n",
            "370 0.31155043840408325 1.0\n",
            "380 0.28158462047576904 0.9833333492279053\n",
            "390 0.3217686414718628 0.9833333492279053\n",
            "test: 0.793999969959259\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkQPWjlVh1m_"
      },
      "source": [
        "## Spektral"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OOO5pECoh4Zw",
        "outputId": "e427d5f7-6ee3-466e-89d0-73281334b48c"
      },
      "source": [
        "!pip install spektral"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting spektral\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/02/5eb4c9e7e4b926093c127d00acdc83810b5ac77eeba32b6d5ae4eadd1da3/spektral-1.0.6-py3-none-any.whl (114kB)\n",
            "\r\u001b[K     |██▉                             | 10kB 19.8MB/s eta 0:00:01\r\u001b[K     |█████▊                          | 20kB 16.6MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 30kB 13.9MB/s eta 0:00:01\r\u001b[K     |███████████▌                    | 40kB 12.9MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 51kB 7.1MB/s eta 0:00:01\r\u001b[K     |█████████████████▏              | 61kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 71kB 8.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 81kB 8.7MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 92kB 8.2MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▋   | 102kB 7.0MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 112kB 7.0MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 122kB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from spektral) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.19.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from spektral) (0.22.2.post1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from spektral) (4.41.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from spektral) (1.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from spektral) (1.1.5)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from spektral) (1.0.1)\n",
            "Requirement already satisfied: tensorflow>=2.1.0 in /usr/local/lib/python3.7/dist-packages (from spektral) (2.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from spektral) (2.5.1)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.7/dist-packages (from spektral) (4.2.6)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->spektral) (2020.12.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->spektral) (2018.9)\n",
            "Requirement already satisfied: tensorboard~=2.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.5.0)\n",
            "Requirement already satisfied: keras-nightly~=2.5.0.dev in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.5.0.dev2021032900)\n",
            "Requirement already satisfied: wheel~=0.35 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.36.2)\n",
            "Requirement already satisfied: h5py~=3.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: opt-einsum~=3.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.3.0)\n",
            "Requirement already satisfied: flatbuffers~=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.12)\n",
            "Requirement already satisfied: google-pasta~=0.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.2.0)\n",
            "Requirement already satisfied: grpcio~=1.34.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.34.1)\n",
            "Requirement already satisfied: astunparse~=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.6.3)\n",
            "Requirement already satisfied: wrapt~=1.12.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing~=1.1.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.2)\n",
            "Requirement already satisfied: six~=1.15.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.15.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.6.0,>=2.5.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (2.5.0)\n",
            "Requirement already satisfied: typing-extensions~=3.7.4 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.7.4.3)\n",
            "Requirement already satisfied: gast==0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.4.0)\n",
            "Requirement already satisfied: absl-py~=0.10 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (3.12.4)\n",
            "Requirement already satisfied: termcolor~=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow>=2.1.0->spektral) (1.1.0)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx->spektral) (4.4.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (3.3.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.8.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (56.1.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.4.4)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.6.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.30.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py~=3.1.0->tensorflow>=2.1.0->spektral) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (4.0.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (1.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3.6\" in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (4.7.2)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (4.2.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.2.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (3.4.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.7/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3.6\"->google-auth<2,>=1.6.3->tensorboard~=2.5->tensorflow>=2.1.0->spektral) (0.4.8)\n",
            "Installing collected packages: spektral\n",
            "Successfully installed spektral-1.0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QdIftScah8qa",
        "outputId": "6f38b93b-9293-405a-b173-706f2246d1ce"
      },
      "source": [
        "!git clone https://github.com/danielegrattarola/spektral.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'spektral'...\n",
            "remote: Enumerating objects: 7359, done.\u001b[K\n",
            "remote: Counting objects: 100% (284/284), done.\u001b[K\n",
            "remote: Compressing objects: 100% (193/193), done.\u001b[K\n",
            "remote: Total 7359 (delta 164), reused 160 (delta 80), pack-reused 7075\u001b[K\n",
            "Receiving objects: 100% (7359/7359), 12.26 MiB | 20.25 MiB/s, done.\n",
            "Resolving deltas: 100% (4537/4537), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTmeWCxwiCzZ",
        "outputId": "faa0b6db-c433-4e9d-985e-1eb57a40376b"
      },
      "source": [
        "%cd spektral/examples/node_prediction/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/spektral/examples/node_prediction\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmXMTGjpiIxN",
        "outputId": "2cc40a6d-c0f2-4492-805e-082d1d9da619"
      },
      "source": [
        "!python citation_gcn.py"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-06-01 09:48:46.923295: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "Pre-processing node features\n",
            "/usr/local/lib/python3.7/dist-packages/scipy/sparse/_index.py:126: SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. lil_matrix is more efficient.\n",
            "  self._set_arrayXarray(i, j, x)\n",
            "2021-06-01 09:48:49.838535: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
            "2021-06-01 09:48:49.866615: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:49.867240: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-06-01 09:48:49.867283: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-06-01 09:48:49.869159: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-06-01 09:48:49.869253: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "2021-06-01 09:48:49.870653: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
            "2021-06-01 09:48:49.871040: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
            "2021-06-01 09:48:49.872454: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.10\n",
            "2021-06-01 09:48:49.872925: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
            "2021-06-01 09:48:49.873114: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
            "2021-06-01 09:48:49.873215: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:49.873826: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:49.874355: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-06-01 09:48:49.874899: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:49.875438: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
            "pciBusID: 0000:00:04.0 name: Tesla T4 computeCapability: 7.5\n",
            "coreClock: 1.59GHz coreCount: 40 deviceMemorySize: 14.75GiB deviceMemoryBandwidth: 298.08GiB/s\n",
            "2021-06-01 09:48:49.875515: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:49.876112: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:49.876626: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0\n",
            "2021-06-01 09:48:49.876673: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-06-01 09:48:50.457558: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
            "2021-06-01 09:48:50.457605: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 \n",
            "2021-06-01 09:48:50.457624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N \n",
            "2021-06-01 09:48:50.457843: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:50.458754: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:50.459622: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
            "2021-06-01 09:48:50.460227: W tensorflow/core/common_runtime/gpu/gpu_bfc_allocator.cc:39] Overriding allow_growth setting because the TF_FORCE_GPU_ALLOW_GROWTH environment variable is set. Original config value was 0.\n",
            "2021-06-01 09:48:50.460279: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 13837 MB memory) -> physical GPU (device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5)\n",
            "Epoch 1/200\n",
            "2021-06-01 09:48:51.655535: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
            "2021-06-01 09:48:51.657901: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2199995000 Hz\n",
            "2021-06-01 09:48:51.719260: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
            "2021-06-01 09:48:52.213068: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.1087 - acc: 0.2833 - val_loss: 1.1007 - val_acc: 0.6680\n",
            "Epoch 2/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.1000 - acc: 0.6667 - val_loss: 1.0971 - val_acc: 0.7420\n",
            "Epoch 3/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.0945 - acc: 0.7500 - val_loss: 1.0922 - val_acc: 0.7620\n",
            "Epoch 4/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0896 - acc: 0.8000 - val_loss: 1.0870 - val_acc: 0.7560\n",
            "Epoch 5/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.0819 - acc: 0.8167 - val_loss: 1.0812 - val_acc: 0.7440\n",
            "Epoch 6/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 1.0748 - acc: 0.8667 - val_loss: 1.0756 - val_acc: 0.7520\n",
            "Epoch 7/200\n",
            "1/1 [==============================] - 0s 72ms/step - loss: 1.0669 - acc: 0.9000 - val_loss: 1.0702 - val_acc: 0.7480\n",
            "Epoch 8/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 1.0588 - acc: 0.8833 - val_loss: 1.0644 - val_acc: 0.7460\n",
            "Epoch 9/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 1.0476 - acc: 0.8667 - val_loss: 1.0581 - val_acc: 0.7460\n",
            "Epoch 10/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.0439 - acc: 0.8667 - val_loss: 1.0523 - val_acc: 0.7480\n",
            "Epoch 11/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 1.0301 - acc: 0.8833 - val_loss: 1.0457 - val_acc: 0.7600\n",
            "Epoch 12/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 1.0192 - acc: 0.9000 - val_loss: 1.0395 - val_acc: 0.7580\n",
            "Epoch 13/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 1.0042 - acc: 0.9000 - val_loss: 1.0337 - val_acc: 0.7640\n",
            "Epoch 14/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.9934 - acc: 0.9500 - val_loss: 1.0267 - val_acc: 0.7660\n",
            "Epoch 15/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.9834 - acc: 0.9167 - val_loss: 1.0196 - val_acc: 0.7700\n",
            "Epoch 16/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.9824 - acc: 0.9333 - val_loss: 1.0132 - val_acc: 0.7660\n",
            "Epoch 17/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.9568 - acc: 0.9500 - val_loss: 1.0059 - val_acc: 0.7640\n",
            "Epoch 18/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.9458 - acc: 0.9333 - val_loss: 0.9985 - val_acc: 0.7640\n",
            "Epoch 19/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.9329 - acc: 0.9500 - val_loss: 0.9910 - val_acc: 0.7620\n",
            "Epoch 20/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.9165 - acc: 0.9167 - val_loss: 0.9834 - val_acc: 0.7600\n",
            "Epoch 21/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.9198 - acc: 0.9000 - val_loss: 0.9758 - val_acc: 0.7660\n",
            "Epoch 22/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.8942 - acc: 0.9000 - val_loss: 0.9695 - val_acc: 0.7640\n",
            "Epoch 23/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.8700 - acc: 0.8667 - val_loss: 0.9623 - val_acc: 0.7660\n",
            "Epoch 24/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.8504 - acc: 0.8833 - val_loss: 0.9548 - val_acc: 0.7660\n",
            "Epoch 25/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.8465 - acc: 0.9333 - val_loss: 0.9476 - val_acc: 0.7720\n",
            "Epoch 26/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.8508 - acc: 0.9167 - val_loss: 0.9405 - val_acc: 0.7680\n",
            "Epoch 27/200\n",
            "1/1 [==============================] - 0s 76ms/step - loss: 0.8251 - acc: 0.9000 - val_loss: 0.9332 - val_acc: 0.7740\n",
            "Epoch 28/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.8203 - acc: 0.9000 - val_loss: 0.9272 - val_acc: 0.7720\n",
            "Epoch 29/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.8212 - acc: 0.9167 - val_loss: 0.9206 - val_acc: 0.7700\n",
            "Epoch 30/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.7902 - acc: 0.9167 - val_loss: 0.9143 - val_acc: 0.7720\n",
            "Epoch 31/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.7612 - acc: 0.9667 - val_loss: 0.9083 - val_acc: 0.7700\n",
            "Epoch 32/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.7885 - acc: 0.9333 - val_loss: 0.9023 - val_acc: 0.7640\n",
            "Epoch 33/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.7527 - acc: 0.9500 - val_loss: 0.8949 - val_acc: 0.7640\n",
            "Epoch 34/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.7442 - acc: 0.9167 - val_loss: 0.8896 - val_acc: 0.7640\n",
            "Epoch 35/200\n",
            "1/1 [==============================] - 0s 48ms/step - loss: 0.7437 - acc: 0.9333 - val_loss: 0.8838 - val_acc: 0.7660\n",
            "Epoch 36/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.7222 - acc: 0.9333 - val_loss: 0.8795 - val_acc: 0.7700\n",
            "Epoch 37/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.7130 - acc: 0.9500 - val_loss: 0.8737 - val_acc: 0.7720\n",
            "Epoch 38/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.6836 - acc: 0.9500 - val_loss: 0.8684 - val_acc: 0.7700\n",
            "Epoch 39/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6734 - acc: 0.9500 - val_loss: 0.8632 - val_acc: 0.7700\n",
            "Epoch 40/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6580 - acc: 0.9667 - val_loss: 0.8578 - val_acc: 0.7740\n",
            "Epoch 41/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6526 - acc: 0.9500 - val_loss: 0.8522 - val_acc: 0.7720\n",
            "Epoch 42/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.6601 - acc: 0.9167 - val_loss: 0.8473 - val_acc: 0.7640\n",
            "Epoch 43/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.6387 - acc: 0.9333 - val_loss: 0.8433 - val_acc: 0.7680\n",
            "Epoch 44/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.6156 - acc: 0.9500 - val_loss: 0.8403 - val_acc: 0.7740\n",
            "Epoch 45/200\n",
            "1/1 [==============================] - 0s 70ms/step - loss: 0.6385 - acc: 0.9167 - val_loss: 0.8362 - val_acc: 0.7700\n",
            "Epoch 46/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.6098 - acc: 0.9667 - val_loss: 0.8318 - val_acc: 0.7740\n",
            "Epoch 47/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.6002 - acc: 0.9333 - val_loss: 0.8284 - val_acc: 0.7720\n",
            "Epoch 48/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.5755 - acc: 0.9667 - val_loss: 0.8248 - val_acc: 0.7720\n",
            "Epoch 49/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.5812 - acc: 0.9500 - val_loss: 0.8211 - val_acc: 0.7720\n",
            "Epoch 50/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.5860 - acc: 0.9333 - val_loss: 0.8172 - val_acc: 0.7740\n",
            "Epoch 51/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.5879 - acc: 0.9500 - val_loss: 0.8147 - val_acc: 0.7720\n",
            "Epoch 52/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.5406 - acc: 0.9667 - val_loss: 0.8122 - val_acc: 0.7800\n",
            "Epoch 53/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.5848 - acc: 0.9500 - val_loss: 0.8092 - val_acc: 0.7800\n",
            "Epoch 54/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.5711 - acc: 0.9500 - val_loss: 0.8063 - val_acc: 0.7800\n",
            "Epoch 55/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.5655 - acc: 0.9167 - val_loss: 0.8052 - val_acc: 0.7800\n",
            "Epoch 56/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.5535 - acc: 0.9667 - val_loss: 0.8021 - val_acc: 0.7800\n",
            "Epoch 57/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.5376 - acc: 0.9833 - val_loss: 0.7994 - val_acc: 0.7800\n",
            "Epoch 58/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.5529 - acc: 0.9500 - val_loss: 0.7954 - val_acc: 0.7820\n",
            "Epoch 59/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.5444 - acc: 0.9500 - val_loss: 0.7937 - val_acc: 0.7800\n",
            "Epoch 60/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.5188 - acc: 0.9667 - val_loss: 0.7904 - val_acc: 0.7820\n",
            "Epoch 61/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.5184 - acc: 0.9500 - val_loss: 0.7876 - val_acc: 0.7840\n",
            "Epoch 62/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.5445 - acc: 0.9167 - val_loss: 0.7865 - val_acc: 0.7840\n",
            "Epoch 63/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.5127 - acc: 0.9500 - val_loss: 0.7849 - val_acc: 0.7780\n",
            "Epoch 64/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4882 - acc: 0.9667 - val_loss: 0.7819 - val_acc: 0.7800\n",
            "Epoch 65/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.5014 - acc: 0.9667 - val_loss: 0.7809 - val_acc: 0.7780\n",
            "Epoch 66/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4754 - acc: 0.9500 - val_loss: 0.7796 - val_acc: 0.7800\n",
            "Epoch 67/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.4824 - acc: 0.9667 - val_loss: 0.7776 - val_acc: 0.7800\n",
            "Epoch 68/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4629 - acc: 0.9833 - val_loss: 0.7767 - val_acc: 0.7780\n",
            "Epoch 69/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4934 - acc: 0.9833 - val_loss: 0.7754 - val_acc: 0.7800\n",
            "Epoch 70/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4940 - acc: 0.9667 - val_loss: 0.7728 - val_acc: 0.7820\n",
            "Epoch 71/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4755 - acc: 0.9333 - val_loss: 0.7724 - val_acc: 0.7820\n",
            "Epoch 72/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4460 - acc: 0.9667 - val_loss: 0.7714 - val_acc: 0.7780\n",
            "Epoch 73/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4939 - acc: 0.9667 - val_loss: 0.7668 - val_acc: 0.7880\n",
            "Epoch 74/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4328 - acc: 0.9833 - val_loss: 0.7648 - val_acc: 0.7900\n",
            "Epoch 75/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4672 - acc: 0.9667 - val_loss: 0.7629 - val_acc: 0.7940\n",
            "Epoch 76/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4408 - acc: 0.9667 - val_loss: 0.7617 - val_acc: 0.7900\n",
            "Epoch 77/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4921 - acc: 0.9500 - val_loss: 0.7618 - val_acc: 0.7900\n",
            "Epoch 78/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4573 - acc: 0.9833 - val_loss: 0.7588 - val_acc: 0.7920\n",
            "Epoch 79/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4382 - acc: 0.9833 - val_loss: 0.7592 - val_acc: 0.7900\n",
            "Epoch 80/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4324 - acc: 1.0000 - val_loss: 0.7568 - val_acc: 0.7920\n",
            "Epoch 81/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4426 - acc: 0.9500 - val_loss: 0.7544 - val_acc: 0.7900\n",
            "Epoch 82/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4499 - acc: 0.9333 - val_loss: 0.7541 - val_acc: 0.7880\n",
            "Epoch 83/200\n",
            "1/1 [==============================] - 0s 73ms/step - loss: 0.4306 - acc: 0.9500 - val_loss: 0.7547 - val_acc: 0.7820\n",
            "Epoch 84/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4389 - acc: 0.9667 - val_loss: 0.7557 - val_acc: 0.7860\n",
            "Epoch 85/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4576 - acc: 0.9500 - val_loss: 0.7538 - val_acc: 0.7880\n",
            "Epoch 86/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4255 - acc: 0.9500 - val_loss: 0.7531 - val_acc: 0.7880\n",
            "Epoch 87/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4108 - acc: 0.9667 - val_loss: 0.7523 - val_acc: 0.7860\n",
            "Epoch 88/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4365 - acc: 0.9667 - val_loss: 0.7504 - val_acc: 0.7860\n",
            "Epoch 89/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4176 - acc: 1.0000 - val_loss: 0.7490 - val_acc: 0.7800\n",
            "Epoch 90/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4193 - acc: 0.9667 - val_loss: 0.7470 - val_acc: 0.7900\n",
            "Epoch 91/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4121 - acc: 0.9667 - val_loss: 0.7472 - val_acc: 0.7920\n",
            "Epoch 92/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.4051 - acc: 1.0000 - val_loss: 0.7446 - val_acc: 0.7920\n",
            "Epoch 93/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.4259 - acc: 0.9667 - val_loss: 0.7440 - val_acc: 0.7900\n",
            "Epoch 94/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.4162 - acc: 1.0000 - val_loss: 0.7424 - val_acc: 0.7920\n",
            "Epoch 95/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.4394 - acc: 0.9500 - val_loss: 0.7407 - val_acc: 0.7980\n",
            "Epoch 96/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.4106 - acc: 0.9667 - val_loss: 0.7389 - val_acc: 0.7940\n",
            "Epoch 97/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3763 - acc: 0.9833 - val_loss: 0.7385 - val_acc: 0.7900\n",
            "Epoch 98/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3839 - acc: 1.0000 - val_loss: 0.7361 - val_acc: 0.7980\n",
            "Epoch 99/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.4097 - acc: 0.9833 - val_loss: 0.7349 - val_acc: 0.7960\n",
            "Epoch 100/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3556 - acc: 0.9833 - val_loss: 0.7351 - val_acc: 0.7920\n",
            "Epoch 101/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3845 - acc: 0.9667 - val_loss: 0.7349 - val_acc: 0.7980\n",
            "Epoch 102/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3836 - acc: 0.9833 - val_loss: 0.7327 - val_acc: 0.7960\n",
            "Epoch 103/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3978 - acc: 0.9833 - val_loss: 0.7317 - val_acc: 0.7940\n",
            "Epoch 104/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4366 - acc: 0.9833 - val_loss: 0.7295 - val_acc: 0.7980\n",
            "Epoch 105/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3762 - acc: 0.9833 - val_loss: 0.7299 - val_acc: 0.7900\n",
            "Epoch 106/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3634 - acc: 0.9833 - val_loss: 0.7299 - val_acc: 0.7980\n",
            "Epoch 107/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3906 - acc: 0.9667 - val_loss: 0.7308 - val_acc: 0.7920\n",
            "Epoch 108/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3715 - acc: 1.0000 - val_loss: 0.7295 - val_acc: 0.7920\n",
            "Epoch 109/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3472 - acc: 1.0000 - val_loss: 0.7272 - val_acc: 0.7940\n",
            "Epoch 110/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3495 - acc: 1.0000 - val_loss: 0.7259 - val_acc: 0.7940\n",
            "Epoch 111/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.4014 - acc: 0.9667 - val_loss: 0.7257 - val_acc: 0.7920\n",
            "Epoch 112/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3683 - acc: 0.9667 - val_loss: 0.7259 - val_acc: 0.7900\n",
            "Epoch 113/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3777 - acc: 0.9667 - val_loss: 0.7277 - val_acc: 0.7840\n",
            "Epoch 114/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3587 - acc: 1.0000 - val_loss: 0.7248 - val_acc: 0.7880\n",
            "Epoch 115/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3850 - acc: 0.9667 - val_loss: 0.7229 - val_acc: 0.7900\n",
            "Epoch 116/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3515 - acc: 1.0000 - val_loss: 0.7211 - val_acc: 0.7900\n",
            "Epoch 117/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3480 - acc: 0.9667 - val_loss: 0.7214 - val_acc: 0.7880\n",
            "Epoch 118/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3635 - acc: 0.9833 - val_loss: 0.7227 - val_acc: 0.7900\n",
            "Epoch 119/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3679 - acc: 0.9833 - val_loss: 0.7233 - val_acc: 0.7880\n",
            "Epoch 120/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.3589 - acc: 1.0000 - val_loss: 0.7213 - val_acc: 0.7880\n",
            "Epoch 121/200\n",
            "1/1 [==============================] - 0s 63ms/step - loss: 0.3196 - acc: 1.0000 - val_loss: 0.7198 - val_acc: 0.7920\n",
            "Epoch 122/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3645 - acc: 0.9833 - val_loss: 0.7197 - val_acc: 0.7900\n",
            "Epoch 123/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3732 - acc: 0.9667 - val_loss: 0.7185 - val_acc: 0.7860\n",
            "Epoch 124/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3528 - acc: 0.9833 - val_loss: 0.7172 - val_acc: 0.7860\n",
            "Epoch 125/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.3213 - acc: 1.0000 - val_loss: 0.7159 - val_acc: 0.7900\n",
            "Epoch 126/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3225 - acc: 0.9833 - val_loss: 0.7137 - val_acc: 0.7880\n",
            "Epoch 127/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.4003 - acc: 0.9833 - val_loss: 0.7155 - val_acc: 0.7880\n",
            "Epoch 128/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3505 - acc: 0.9667 - val_loss: 0.7136 - val_acc: 0.7840\n",
            "Epoch 129/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3589 - acc: 1.0000 - val_loss: 0.7124 - val_acc: 0.7920\n",
            "Epoch 130/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3133 - acc: 0.9833 - val_loss: 0.7124 - val_acc: 0.7880\n",
            "Epoch 131/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3548 - acc: 0.9833 - val_loss: 0.7123 - val_acc: 0.7880\n",
            "Epoch 132/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3521 - acc: 0.9667 - val_loss: 0.7133 - val_acc: 0.7860\n",
            "Epoch 133/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3164 - acc: 1.0000 - val_loss: 0.7110 - val_acc: 0.7920\n",
            "Epoch 134/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3051 - acc: 1.0000 - val_loss: 0.7088 - val_acc: 0.7920\n",
            "Epoch 135/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3354 - acc: 0.9833 - val_loss: 0.7074 - val_acc: 0.7920\n",
            "Epoch 136/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3251 - acc: 0.9833 - val_loss: 0.7078 - val_acc: 0.7880\n",
            "Epoch 137/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3472 - acc: 0.9833 - val_loss: 0.7076 - val_acc: 0.7840\n",
            "Epoch 138/200\n",
            "1/1 [==============================] - 0s 58ms/step - loss: 0.3361 - acc: 0.9667 - val_loss: 0.7088 - val_acc: 0.7840\n",
            "Epoch 139/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3117 - acc: 1.0000 - val_loss: 0.7079 - val_acc: 0.7920\n",
            "Epoch 140/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3095 - acc: 0.9833 - val_loss: 0.7088 - val_acc: 0.7840\n",
            "Epoch 141/200\n",
            "1/1 [==============================] - 0s 49ms/step - loss: 0.3409 - acc: 0.9833 - val_loss: 0.7066 - val_acc: 0.7880\n",
            "Epoch 142/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3220 - acc: 1.0000 - val_loss: 0.7074 - val_acc: 0.7860\n",
            "Epoch 143/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.3474 - acc: 0.9667 - val_loss: 0.7066 - val_acc: 0.7860\n",
            "Epoch 144/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3192 - acc: 1.0000 - val_loss: 0.7057 - val_acc: 0.7900\n",
            "Epoch 145/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3145 - acc: 1.0000 - val_loss: 0.7066 - val_acc: 0.7900\n",
            "Epoch 146/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.3057 - acc: 0.9833 - val_loss: 0.7052 - val_acc: 0.7800\n",
            "Epoch 147/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3135 - acc: 1.0000 - val_loss: 0.7044 - val_acc: 0.7840\n",
            "Epoch 148/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3207 - acc: 1.0000 - val_loss: 0.6993 - val_acc: 0.7940\n",
            "Epoch 149/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3433 - acc: 0.9833 - val_loss: 0.6982 - val_acc: 0.7960\n",
            "Epoch 150/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3247 - acc: 1.0000 - val_loss: 0.6974 - val_acc: 0.7980\n",
            "Epoch 151/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3260 - acc: 0.9667 - val_loss: 0.6972 - val_acc: 0.8020\n",
            "Epoch 152/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3539 - acc: 0.9833 - val_loss: 0.6989 - val_acc: 0.7920\n",
            "Epoch 153/200\n",
            "1/1 [==============================] - 0s 51ms/step - loss: 0.3031 - acc: 1.0000 - val_loss: 0.6998 - val_acc: 0.7920\n",
            "Epoch 154/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3539 - acc: 0.9833 - val_loss: 0.7017 - val_acc: 0.7920\n",
            "Epoch 155/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.2860 - acc: 1.0000 - val_loss: 0.6982 - val_acc: 0.7960\n",
            "Epoch 156/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3215 - acc: 1.0000 - val_loss: 0.6960 - val_acc: 0.7940\n",
            "Epoch 157/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3191 - acc: 1.0000 - val_loss: 0.6998 - val_acc: 0.7860\n",
            "Epoch 158/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3001 - acc: 1.0000 - val_loss: 0.6993 - val_acc: 0.7840\n",
            "Epoch 159/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3348 - acc: 0.9500 - val_loss: 0.6965 - val_acc: 0.7900\n",
            "Epoch 160/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3319 - acc: 0.9833 - val_loss: 0.6968 - val_acc: 0.7920\n",
            "Epoch 161/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3203 - acc: 1.0000 - val_loss: 0.6975 - val_acc: 0.7920\n",
            "Epoch 162/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.3137 - acc: 0.9833 - val_loss: 0.6971 - val_acc: 0.7940\n",
            "Epoch 163/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.3290 - acc: 0.9833 - val_loss: 0.6976 - val_acc: 0.7900\n",
            "Epoch 164/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2966 - acc: 0.9833 - val_loss: 0.6962 - val_acc: 0.7900\n",
            "Epoch 165/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3063 - acc: 1.0000 - val_loss: 0.6952 - val_acc: 0.7900\n",
            "Epoch 166/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3058 - acc: 0.9667 - val_loss: 0.6961 - val_acc: 0.7900\n",
            "Epoch 167/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2852 - acc: 1.0000 - val_loss: 0.6944 - val_acc: 0.7920\n",
            "Epoch 168/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2882 - acc: 0.9833 - val_loss: 0.6912 - val_acc: 0.7900\n",
            "Epoch 169/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.3166 - acc: 1.0000 - val_loss: 0.6885 - val_acc: 0.7940\n",
            "Epoch 170/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.3198 - acc: 0.9833 - val_loss: 0.6901 - val_acc: 0.7920\n",
            "Epoch 171/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.3161 - acc: 0.9833 - val_loss: 0.6916 - val_acc: 0.7920\n",
            "Epoch 172/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.2879 - acc: 0.9833 - val_loss: 0.6899 - val_acc: 0.7960\n",
            "Epoch 173/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2818 - acc: 0.9833 - val_loss: 0.6872 - val_acc: 0.7920\n",
            "Epoch 174/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2932 - acc: 1.0000 - val_loss: 0.6867 - val_acc: 0.7920\n",
            "Epoch 175/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3048 - acc: 0.9833 - val_loss: 0.6912 - val_acc: 0.7920\n",
            "Epoch 176/200\n",
            "1/1 [==============================] - 0s 52ms/step - loss: 0.3186 - acc: 0.9833 - val_loss: 0.6891 - val_acc: 0.7940\n",
            "Epoch 177/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2985 - acc: 1.0000 - val_loss: 0.6894 - val_acc: 0.7920\n",
            "Epoch 178/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.3094 - acc: 0.9833 - val_loss: 0.6921 - val_acc: 0.7860\n",
            "Epoch 179/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2769 - acc: 1.0000 - val_loss: 0.6906 - val_acc: 0.7880\n",
            "Epoch 180/200\n",
            "1/1 [==============================] - 0s 60ms/step - loss: 0.2909 - acc: 0.9833 - val_loss: 0.6928 - val_acc: 0.7820\n",
            "Epoch 181/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2825 - acc: 0.9833 - val_loss: 0.6898 - val_acc: 0.7920\n",
            "Epoch 182/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2990 - acc: 0.9500 - val_loss: 0.6861 - val_acc: 0.7900\n",
            "Epoch 183/200\n",
            "1/1 [==============================] - 0s 57ms/step - loss: 0.2719 - acc: 1.0000 - val_loss: 0.6846 - val_acc: 0.7880\n",
            "Epoch 184/200\n",
            "1/1 [==============================] - 0s 56ms/step - loss: 0.3037 - acc: 0.9833 - val_loss: 0.6859 - val_acc: 0.7900\n",
            "Epoch 185/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.3051 - acc: 1.0000 - val_loss: 0.6865 - val_acc: 0.7860\n",
            "Epoch 186/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2835 - acc: 0.9833 - val_loss: 0.6856 - val_acc: 0.7880\n",
            "Epoch 187/200\n",
            "1/1 [==============================] - 0s 55ms/step - loss: 0.2969 - acc: 0.9667 - val_loss: 0.6877 - val_acc: 0.7900\n",
            "Epoch 188/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2735 - acc: 1.0000 - val_loss: 0.6869 - val_acc: 0.7900\n",
            "Epoch 189/200\n",
            "1/1 [==============================] - 0s 59ms/step - loss: 0.3137 - acc: 0.9667 - val_loss: 0.6871 - val_acc: 0.7900\n",
            "Epoch 190/200\n",
            "1/1 [==============================] - 0s 50ms/step - loss: 0.2845 - acc: 1.0000 - val_loss: 0.6884 - val_acc: 0.7880\n",
            "Epoch 191/200\n",
            "1/1 [==============================] - 0s 54ms/step - loss: 0.2733 - acc: 0.9833 - val_loss: 0.6884 - val_acc: 0.7880\n",
            "Epoch 192/200\n",
            "1/1 [==============================] - 0s 53ms/step - loss: 0.2748 - acc: 1.0000 - val_loss: 0.6879 - val_acc: 0.7840\n",
            "Epoch 193/200\n",
            "1/1 [==============================] - 0s 61ms/step - loss: 0.2869 - acc: 1.0000 - val_loss: 0.6862 - val_acc: 0.7900\n",
            "Evaluating model.\n",
            "1/1 [==============================] - 0s 41ms/step - loss: 0.6944 - acc: 0.7980\n",
            "Done.\n",
            "Test loss: 0.6944257616996765\n",
            "Test accuracy: 0.7979998588562012\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OIndIS7inNu"
      },
      "source": [
        "4\"\"\"\n",
        "citation_gcn.py\n",
        "\"\"\"\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.losses import CategoricalCrossentropy\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "from spektral.data.loaders import SingleLoader\n",
        "from spektral.datasets.citation import Citation\n",
        "from spektral.layers import GCNConv\n",
        "from spektral.models.gcn import GCN\n",
        "from spektral.transforms import AdjToSpTensor, LayerPreprocess\n",
        "\n",
        "learning_rate = 1e-2  # 학습률\n",
        "seed = 0\n",
        "epochs = 200          # 데이터를 몇 번 사용해서 학습할 건지\n",
        "patience = 10\n",
        "data = \"cora\"\n",
        "\n",
        "tf.random.set_seed(seed=seed)  # seed 고정\n",
        "\n",
        "'''\n",
        "Load data - datasets/citation.py\n",
        "dataset name = cora\n",
        "normalize true = normalize the features\n",
        "transforms\n",
        "transforms/layer_preprocess.py & adj_to_sp_tensor.py\n",
        "LayerPreprocess(GCNConv) - preprocess 기능을 인접행렬에 적용\n",
        "AdjToSpTensor()] - 인접행렬을 sparse tensor로 변환\n",
        "'''\n",
        "dataset = Citation(\n",
        "    data, normalize_x=True, transforms=[LayerPreprocess(GCNConv), AdjToSpTensor()]\n",
        ")\n",
        "\n",
        "\n",
        "# 노드에 대한 avg_loss를 계산하기 위해 binary masks를 sample weights로 변환\n",
        "def mask_to_weights(mask):\n",
        "    return mask.astype(np.float32) / np.count_nonzero(mask)\n",
        "    # mask의 0이 아닌 값의 개수\n",
        "\n",
        "\n",
        "weights_tr, weights_va, weights_te = (\n",
        "    mask_to_weights(mask)\n",
        "    for mask in (dataset.mask_tr, dataset.mask_va, dataset.mask_te)\n",
        ")\n",
        "\n",
        "# architecture 구현\n",
        "# n_labels: number of channels in output\n",
        "# n_input_channels: number of input channels, required for tf 2.1\n",
        "\n",
        "model = GCN(n_labels=dataset.n_labels, n_input_channels=dataset.n_node_features)\n",
        "model.compile(\n",
        "    optimizer=Adam(learning_rate),  # 최적화 알고리즘\n",
        "    loss=CategoricalCrossentropy(reduction=\"sum\"),  # 손실함수\n",
        "    weighted_metrics=[\"acc\"], # 학습과 테스트 과정에서 sample_weight이나 class_weight로 가중치를 적용하여 평가할 측정항목의 리스트\n",
        ")\n",
        "\n",
        "# Train model\n",
        "'''\n",
        "single loader - 단일 그래프를 나타내는 Tensor를 생성\n",
        "dataset = citation\n",
        "sample_weights - 주어진 경우 출력에 추가\n",
        "output -  tuple(inputs, labels) or (inputs, labels, sample_weights)\n",
        "'''\n",
        "loader_tr = SingleLoader(dataset, sample_weights=weights_tr)\n",
        "loader_va = SingleLoader(dataset, sample_weights=weights_va)\n",
        "model.fit(\n",
        "    loader_tr.load(),\n",
        "    steps_per_epoch=loader_tr.steps_per_epoch,\n",
        "    validation_data=loader_va.load(),\n",
        "    validation_steps=loader_va.steps_per_epoch,\n",
        "    epochs=epochs,\n",
        "    callbacks=[EarlyStopping(patience=patience, restore_best_weights=True)],\n",
        ")\n",
        "\n",
        "# Evaluate model\n",
        "print(\"Evaluating model.\")\n",
        "loader_te = SingleLoader(dataset, sample_weights=weights_te)\n",
        "eval_results = model.evaluate(loader_te.load(), steps=loader_te.steps_per_epoch)\n",
        "print(\"Done.\\n\" \"Test loss: {}\\n\" \"Test accuracy: {}\".format(*eval_results))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqfsCabTsiC1"
      },
      "source": [
        "# gcn.py\n",
        "import tensorflow as tf\n",
        "\n",
        "from spektral.layers.convolutional import gcn_conv\n",
        "\n",
        "\n",
        "class GCN(tf.keras.Model):\n",
        "    \"\"\"\n",
        "    This model, with its default hyperparameters, implements the architecture\n",
        "    from the paper:\n",
        "\n",
        "    > [Semi-Supervised Classification with Graph Convolutional Networks](https://arxiv.org/abs/1609.02907)<br>\n",
        "    > Thomas N. Kipf and Max Welling\n",
        "\n",
        "    **Mode**: single, disjoint, mixed, batch.\n",
        "\n",
        "    **Input**\n",
        "\n",
        "    - Node features of shape `([batch], n_nodes, n_node_features)`\n",
        "    - Weighted adjacency matrix of shape `([batch], n_nodes, n_nodes)`\n",
        "\n",
        "    **Output**\n",
        "\n",
        "    - Softmax predictions with shape `([batch], n_nodes, n_labels)`.\n",
        "\n",
        "    **Arguments**\n",
        "\n",
        "    - `n_labels`: number of channels in output;\n",
        "    - `channels`: number of channels in first GCNConv layer;\n",
        "    - `activation`: activation of the first GCNConv layer;\n",
        "    - `output_activation`: activation of the second GCNConv layer;\n",
        "    - `use_bias`: whether to add a learnable bias to the two GCNConv layers;\n",
        "    - `dropout_rate`: `rate` used in `Dropout` layers;\n",
        "    - `l2_reg`: l2 regularization strength;\n",
        "    - `n_input_channels`: number of input channels, required for tf 2.1;\n",
        "    - `**kwargs`: passed to `Model.__init__`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        n_labels,\n",
        "        channels=16,\n",
        "        activation=\"relu\",  # 활성화 함수\n",
        "        output_activation=\"softmax\",\n",
        "        use_bias=False,\n",
        "        dropout_rate=0.5,\n",
        "        l2_reg=2.5e-4,\n",
        "        n_input_channels=None,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.n_labels = n_labels\n",
        "        self.channels = channels\n",
        "        self.activation = activation\n",
        "        self.output_activation = output_activation\n",
        "        self.use_bias = use_bias\n",
        "        self.dropout_rate = dropout_rate\n",
        "        self.l2_reg = l2_reg\n",
        "        self.n_input_channels = n_input_channels\n",
        "        reg = tf.keras.regularizers.l2(l2_reg)\n",
        "        self._d0 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self._gcn0 = gcn_conv.GCNConv(\n",
        "            channels, activation=activation, kernel_regularizer=reg, use_bias=use_bias\n",
        "        )\n",
        "        self._d1 = tf.keras.layers.Dropout(dropout_rate)\n",
        "        self._gcn1 = gcn_conv.GCNConv(\n",
        "            n_labels, activation=output_activation, use_bias=use_bias\n",
        "        )\n",
        "\n",
        "        if tf.version.VERSION < \"2.2\":\n",
        "            if n_input_channels is None:\n",
        "                raise ValueError(\"n_input_channels required for tf < 2.2\")\n",
        "            x = tf.keras.Input((n_input_channels,), dtype=tf.float32)\n",
        "            a = tf.keras.Input((None,), dtype=tf.float32, sparse=True)\n",
        "            self._set_inputs((x, a))\n",
        "\n",
        "    def get_config(self):\n",
        "        return dict(\n",
        "            n_labels=self.n_labels,\n",
        "            channels=self.channels,\n",
        "            activation=self.activation,\n",
        "            output_activation=self.output_activation,\n",
        "            use_bias=self.use_bias,\n",
        "            dropout_rate=self.dropout_rate,\n",
        "            l2_reg=self.l2_reg,\n",
        "            n_input_channels=self.n_input_channels,\n",
        "        )\n",
        "\n",
        "    def call(self, inputs):\n",
        "        if len(inputs) == 2:\n",
        "            x, a = inputs\n",
        "        else:\n",
        "            x, a, _ = inputs  # So that the model can be used with DisjointLoader\n",
        "        if self.n_input_channels is None:\n",
        "            self.n_input_channels = x.shape[-1]\n",
        "        else:\n",
        "            assert self.n_input_channels == x.shape[-1]\n",
        "        x = self._d0(x)\n",
        "        x = self._gcn0([x, a])\n",
        "        x = self._d1(x)\n",
        "        return self._gcn1([x, a])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MLSi4qYjMr-1"
      },
      "source": [
        "# citation.py\n",
        "\n",
        "import os\n",
        "import os.path as osp\n",
        "\n",
        "import networkx as nx\n",
        "import numpy as np\n",
        "import requests\n",
        "import scipy.sparse as sp\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from spektral.data import Dataset, Graph\n",
        "from spektral.datasets.utils import DATASET_FOLDER\n",
        "from spektral.utils.io import load_binary\n",
        "\n",
        "\n",
        "class Citation(Dataset):\n",
        "    \"\"\"\n",
        "    The citation datasets Cora, Citeseer and Pubmed.\n",
        "\n",
        "    Node attributes are bag-of-words vectors representing the most common words\n",
        "    in the text document associated to each node.\n",
        "    Two papers are connected if either one cites the other.\n",
        "    Labels represent the subject area of the paper.\n",
        "\n",
        "    The train, test, and validation splits are given as binary masks and are\n",
        "    accessible via the `mask_tr`, `mask_va`, and `mask_te` attributes.\n",
        "\n",
        "    **Arguments**\n",
        "\n",
        "    - `name`: name of the dataset to load (`'cora'`, `'citeseer'`, or\n",
        "    `'pubmed'`);\n",
        "    - `random_split`: if True, return a randomized split (20 nodes per class\n",
        "    for training, 30 nodes per class for validation and the remaining nodes for\n",
        "    testing, as recommended by [Shchur et al. (2018)](https://arxiv.org/abs/1811.05868)).\n",
        "    If False (default), return the \"Planetoid\" public splits defined by\n",
        "    [Yang et al. (2016)](https://arxiv.org/abs/1603.08861).\n",
        "    - `normalize_x`: if True, normalize the features.\n",
        "    - `dtype`: numpy dtype of graph data.\n",
        "    \"\"\"\n",
        "\n",
        "    url = \"https://github.com/tkipf/gcn/raw/master/gcn/data/{}\"\n",
        "    suffixes = [\"x\", \"y\", \"tx\", \"ty\", \"allx\", \"ally\", \"graph\", \"test.index\"]\n",
        "\n",
        "    def __init__(\n",
        "        self, name, random_split=False, normalize_x=False, dtype=np.float32, **kwargs\n",
        "    ):\n",
        "        if hasattr(dtype, \"as_numpy_dtype\"):\n",
        "            # support tf.dtypes\n",
        "            dtype = dtype.as_numpy_dtype\n",
        "        if name.lower() not in self.available_datasets():\n",
        "            raise ValueError(\n",
        "                \"Unknown dataset {}. See {}.available_datasets() for a complete list of\"\n",
        "                \"available datasets.\".format(name, self.__class__.__name__)\n",
        "            )\n",
        "        self.name = name.lower()\n",
        "        self.random_split = random_split\n",
        "        self.normalize_x = normalize_x\n",
        "        self.mask_tr = self.mask_va = self.mask_te = None\n",
        "        self.dtype = dtype\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "    @property\n",
        "    def path(self):\n",
        "        return osp.join(DATASET_FOLDER, \"Citation\", self.name)\n",
        "\n",
        "    def read(self):\n",
        "        objects = [_read_file(self.path, self.name, s) for s in self.suffixes]\n",
        "        objects = [o.A if sp.issparse(o) else o for o in objects]\n",
        "        x, y, tx, ty, allx, ally, graph, idx_te = objects\n",
        "\n",
        "        # Public Planetoid splits. This is the default\n",
        "        idx_tr = np.arange(y.shape[0])\n",
        "        idx_va = np.arange(y.shape[0], y.shape[0] + 500)\n",
        "        idx_te = idx_te.astype(int)\n",
        "        idx_te_sort = np.sort(idx_te)\n",
        "\n",
        "        # Fix disconnected nodes in Citeseer\n",
        "        if self.name == \"citeseer\":\n",
        "            idx_te_len = idx_te.max() - idx_te.min() + 1\n",
        "            tx_ext = np.zeros((idx_te_len, x.shape[1]))\n",
        "            tx_ext[idx_te_sort - idx_te.min(), :] = tx\n",
        "            tx = tx_ext\n",
        "            ty_ext = np.zeros((idx_te_len, y.shape[1]))\n",
        "            ty_ext[idx_te_sort - idx_te.min(), :] = ty\n",
        "            ty = ty_ext\n",
        "\n",
        "        x = np.vstack((allx, tx))\n",
        "        y = np.vstack((ally, ty))\n",
        "        x[idx_te, :] = x[idx_te_sort, :]\n",
        "        y[idx_te, :] = y[idx_te_sort, :]\n",
        "\n",
        "        # Row-normalize the features\n",
        "        if self.normalize_x:\n",
        "            print(\"Pre-processing node features\")\n",
        "            x = _preprocess_features(x)\n",
        "\n",
        "        if self.random_split:\n",
        "            # Throw away public splits and compute random ones like Shchur et al.\n",
        "            indices = np.arange(y.shape[0])\n",
        "            n_classes = y.shape[1]\n",
        "            idx_tr, idx_te, _, y_te = train_test_split(\n",
        "                indices, y, train_size=20 * n_classes, stratify=y\n",
        "            )\n",
        "            idx_va, idx_te = train_test_split(\n",
        "                idx_te, train_size=30 * n_classes, stratify=y_te\n",
        "            )\n",
        "\n",
        "        # Adjacency matrix\n",
        "        a = nx.adjacency_matrix(nx.from_dict_of_lists(graph))  # CSR\n",
        "        a.setdiag(0)\n",
        "        a.eliminate_zeros()\n",
        "\n",
        "        # Train/valid/test masks\n",
        "        self.mask_tr = _idx_to_mask(idx_tr, y.shape[0])\n",
        "        self.mask_va = _idx_to_mask(idx_va, y.shape[0])\n",
        "        self.mask_te = _idx_to_mask(idx_te, y.shape[0])\n",
        "\n",
        "        return [\n",
        "            Graph(\n",
        "                x=x.astype(self.dtype),\n",
        "                a=a.astype(self.dtype),\n",
        "                y=y.astype(self.dtype),\n",
        "            )\n",
        "        ]\n",
        "\n",
        "    def download(self):\n",
        "        print(\"Downloading {} dataset.\".format(self.name))\n",
        "        os.makedirs(self.path, exist_ok=True)\n",
        "        for n in self.suffixes:\n",
        "            f_name = \"ind.{}.{}\".format(self.name, n)\n",
        "            req = requests.get(self.url.format(f_name))\n",
        "            if req.status_code == 404:\n",
        "                raise ValueError(\n",
        "                    \"Cannot download dataset ({} returned 404).\".format(\n",
        "                        self.url.format(f_name)\n",
        "                    )\n",
        "                )\n",
        "            with open(os.path.join(self.path, f_name), \"wb\") as out_file:\n",
        "                out_file.write(req.content)\n",
        "\n",
        "    @staticmethod\n",
        "    def available_datasets():\n",
        "        return [\"cora\", \"citeseer\", \"pubmed\"]\n",
        "\n",
        "\n",
        "class Cora(Citation):\n",
        "    \"\"\"\n",
        "    Alias for `Citation('cora')`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_split=False, normalize_x=False, **kwargs):\n",
        "        super().__init__(\n",
        "            \"cora\", random_split=random_split, normalize_x=normalize_x, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "class Citeseer(Citation):\n",
        "    \"\"\"\n",
        "    Alias for `Citation('citeseer')`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_split=False, normalize_x=False, **kwargs):\n",
        "        super().__init__(\n",
        "            \"citeseer\", random_split=random_split, normalize_x=normalize_x, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "class Pubmed(Citation):\n",
        "    \"\"\"\n",
        "    Alias for `Citation('pubmed')`.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, random_split=False, normalize_x=False, **kwargs):\n",
        "        super().__init__(\n",
        "            \"pubmed\", random_split=random_split, normalize_x=normalize_x, **kwargs\n",
        "        )\n",
        "\n",
        "\n",
        "def _read_file(path, name, suffix):\n",
        "    full_fname = os.path.join(path, \"ind.{}.{}\".format(name, suffix))\n",
        "    if suffix == \"test.index\":\n",
        "        return np.loadtxt(full_fname)\n",
        "\n",
        "    return load_binary(full_fname)\n",
        "\n",
        "\n",
        "def _idx_to_mask(idx, l):\n",
        "    mask = np.zeros(l)\n",
        "    mask[idx] = 1\n",
        "    return np.array(mask, dtype=np.bool)\n",
        "\n",
        "\n",
        "def _preprocess_features(features):\n",
        "    rowsum = np.array(features.sum(1))\n",
        "    r_inv = np.power(rowsum, -1).flatten()\n",
        "    r_inv[np.isinf(r_inv)] = 0.0\n",
        "    r_mat_inv = sp.diags(r_inv)\n",
        "    features = r_mat_inv.dot(features)\n",
        "    return features\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow를 이용한 MNIST 실습 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 핵심은 정확도 비교해보기!\n",
    "#### 1) hidden layer 개수를 조절하면서\n",
    "#### 2) layer 의 node 개수를 조절하면서\n",
    "#### 3) epoch의 수를 조절하면서"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor graph reset 함수\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data/mnist/train-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/train-labels-idx1-ubyte.gz\n",
      "Extracting data/mnist/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/mnist/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets('data/mnist/', one_hot=True)   #이미지 데이터 저장, one_hot incoding하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet at 0x279577ed160>,\n",
       " <tensorflow.contrib.learn.python.learn.datasets.mnist.DataSet at 0x27957957b00>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mnist.train, mnist.test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.float32)\n",
    "y = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "neural networks 구축  \n",
    "1) layer 수 조절해보기  \n",
    "2) layer 마다의 node 수 조절해보기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_1 = tf.Variable(tf.random_normal([784, 256]))   # Weight = 784*256\n",
    "b_1 = tf.Variable(tf.random_normal([256]))        # bias = 256*1\n",
    "hidden_1 = tf.sigmoid(tf.matmul(X, W_1) + b_1)     #hidden layer = X * W_1 + b_1 = WX -> Sigmoid\n",
    "\n",
    "W_2 = tf.Variable(tf.random_normal([256, 10]))\n",
    "b_2 = tf.Variable(tf.random_normal([10]))\n",
    "hypo = tf.sigmoid(tf.matmul(hidden_1, W_2) + b_2)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "W_1 = tf.Variable(tf.random_normal([784, 128]))\n",
    "b_1 = tf.Variable(tf.random_normal([128]))\n",
    "hidden_1 = tf.sigmoid(tf.matmul(X, W_1) + b_1)\n",
    "\n",
    "W_2 = tf.Variable(tf.random_normal([128, 64]))\n",
    "b_2 = tf.Variable(tf.random_normal([64]))\n",
    "hidden_2 = tf.sigmoid(tf.matmul(hidden_1, W_2) + b_2)\n",
    "\n",
    "W_3 = tf.Variable(tf.random_normal([64, 10]))\n",
    "b_3 = tf.Variable(tf.random_normal([10]))\n",
    "hypo = tf.sigmoid(tf.matmul(hidden_2, W_3) + b_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cost = tf.reduce_mean(  # cost 계산\n",
    "    tf.nn.softmax_cross_entropy_with_logits(\n",
    "        logits=hypo, labels=y))\n",
    "# 우리가 이미 살펴봤던 loss\n",
    "# logits = 예측값, lables = 실제값"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "predict : hypo > 0.5 크다면 1로 저장.  \n",
    "hypo > 0.5 -> 논리식이기 때문에, 결과값이 True or False  \n",
    "tf.cast(\"value\", \"변환할 type\")\n",
    ": value를 해당 type으로 형변환 : True -> 1, False -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "accuracy  \n",
    "tf.equal(predict, y) : predict와 y 과 같니? -> 같으면 True, 다르면 False  \n",
    "tf.cast : 1, 0 (array로 저장)\n",
    "tf.reduce_mean : array 평균 : 전체중에 1이 몇 %있니?  \n",
    "ex) [1, 1, 1, 1, 0] : 0.8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = tf.cast(hypo > 0.5, tf.float32) # 1 or 0    // hypo가 0.5보다 크면 true=1, 작으면 false = 0으로 cast해줌\n",
    "accuracy = tf.reduce_mean(tf.cast(tf.equal(predict, y), tf.float32))   #predict랑 y가 같으면 1/다르면0을 모든 data에 해서 그걸 평균냄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3) epoch 수를 조절해보세요."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100  # 몇 개로 나눠서 돌릴꺼야?\n",
    "epoches = 10   # 몇 바퀴 돌릴꺼야?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total_batch :  550\n",
      " 1 : 1.9969321205399224\n",
      " 2 : 1.8408165062557564\n",
      " 3 : 1.7832937249270324\n",
      " 4 : 1.760353579521181\n",
      " 5 : 1.746986901760103\n",
      " 6 : 1.737656547589736\n",
      " 7 : 1.7307951400496768\n",
      " 8 : 1.7253239820220263\n",
      " 9 : 1.7206392470273117\n",
      "10 : 1.7168226200884031\n",
      "Learning finished!\n",
      "accuracy : 0.94932\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # total_batch 수(업데이트 수) = 전체 데이터 개수 // batch크기\n",
    "    total_batch = mnist.train.num_examples//batch_size     # 550 = 55000(n_tr)/100\n",
    "    print('total_batch : ', total_batch)\n",
    "\n",
    "    for epoch in range(epoches):   # epoch개수=10 만큼 돌릴꺼야 -> 총 5500번 update\n",
    "        avg_cost = 0    # avg 초기화\n",
    "\n",
    "        for i in range(total_batch):  # total batch수 만큼 for문 돌려서 optimizer를 누르자\n",
    "            #training dataset\n",
    "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)   # next_batch : 다음 batch로 넘어감\n",
    "\n",
    "\n",
    "            sess.run(optimizer, feed_dict={X: batch_xs, y: batch_ys})\n",
    "            c = sess.run(cost, feed_dict={X: batch_xs, y: batch_ys})\n",
    "\n",
    "            avg_cost += c / total_batch\n",
    "\n",
    "        print('{:2} : {:}'.format(epoch+1, avg_cost))\n",
    "        \n",
    "    print('Learning finished!')\n",
    "\n",
    "    print('accuracy :', sess.run(accuracy, feed_dict={X: mnist.test.images, \n",
    "                                                  y: mnist.test.labels}))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
